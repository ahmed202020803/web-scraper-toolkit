{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraper Toolkit - Google Colab Example\n",
    "\n",
    "This notebook demonstrates how to use the Web Scraper Toolkit in Google Colab. It includes installation, setup, and examples of basic and advanced scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clone the Repository and Install Dependencies\n",
    "\n",
    "First, let's clone the repository and install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/ahmed202020803/web-scraper-toolkit.git\n",
    "!cd web-scraper-toolkit && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up the Environment\n",
    "\n",
    "Now, let's set up the environment by adding the repository to the Python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Add the repository to the Python path\n",
    "sys.path.append('/content/web-scraper-toolkit')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs('/content/web-scraper-toolkit/data', exist_ok=True)\n",
    "os.makedirs('/content/web-scraper-toolkit/config', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sample Configuration Files\n",
    "\n",
    "Let's create the necessary configuration files for the toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample user agents file if it doesn't exist\n",
    "user_agents_path = '/content/web-scraper-toolkit/config/user_agents.txt'\n",
    "if not os.path.exists(user_agents_path):\n",
    "    with open(user_agents_path, 'w') as f:\n",
    "        f.write(\"\"\"\n",
    "# Chrome\n",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\n",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36\n",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.101 Safari/537.36\n",
    "\n",
    "# Firefox\n",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\n",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0\n",
    "Mozilla/5.0 (X11; Linux i686; rv:89.0) Gecko/20100101 Firefox/89.0\n",
    "\n",
    "# Safari\n",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15\n",
    "        \"\"\")\n",
    "    print(\"Created user agents file\")\n",
    "else:\n",
    "    print(\"User agents file already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import the Web Scraper Toolkit\n",
    "\n",
    "Now, let's import the Web Scraper Toolkit and create a scraper instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from web_scraper_toolkit import Scraper, ScraperConfig\n",
    "\n",
    "# Create a scraper with default configuration\n",
    "scraper = Scraper(engine=\"requests\")\n",
    "print(\"Scraper initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Scraping Example\n",
    "\n",
    "Let's start with a basic scraping example to extract information from a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data schema\n",
    "schema = {\n",
    "    \"title\": \"h1\",\n",
    "    \"description\": \"meta[name='description']\",\n",
    "    \"paragraphs\": {\n",
    "        \"selector\": \"p\",\n",
    "        \"multiple\": True\n",
    "    },\n",
    "    \"links\": {\n",
    "        \"selector\": \"a\",\n",
    "        \"attribute\": \"href\",\n",
    "        \"multiple\": True\n",
    "    },\n",
    "    \"images\": {\n",
    "        \"selector\": \"img\",\n",
    "        \"attribute\": \"src\",\n",
    "        \"multiple\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Scrape a website\n",
    "print(\"Scraping example.com...\")\n",
    "data = scraper.scrape(\"https://example.com\", schema)\n",
    "\n",
    "# Print the scraped data\n",
    "print(\"\\nScraped Data:\")\n",
    "print(f\"Title: {data.get('title')}\")\n",
    "print(f\"Description: {data.get('description')}\")\n",
    "print(f\"Number of paragraphs: {len(data.get('paragraphs', []))}\")\n",
    "print(f\"Number of links: {len(data.get('links', []))}\")\n",
    "print(f\"Number of images: {len(data.get('images', []))}\")\n",
    "\n",
    "# Export the data to various formats\n",
    "print(\"\\nExporting data...\")\n",
    "scraper.export(data, \"/content/web-scraper-toolkit/data/example.json\")\n",
    "scraper.export(data, \"/content/web-scraper-toolkit/data/example.csv\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View the Exported Data\n",
    "\n",
    "Let's view the exported data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the JSON file\n",
    "import json\n",
    "with open(\"/content/web-scraper-toolkit/data/example.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "print(\"JSON Data:\")\n",
    "print(json.dumps(json_data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the CSV file\n",
    "import pandas as pd\n",
    "csv_data = pd.read_csv(\"/content/web-scraper-toolkit/data/example.csv\")\n",
    "\n",
    "print(\"CSV Data:\")\n",
    "csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Scraping Example\n",
    "\n",
    "Now, let's try a more advanced scraping example with custom configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom configuration\n",
    "config = ScraperConfig(\n",
    "    engine=\"requests\",\n",
    "    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    user_agent_rotation=True,\n",
    "    respect_robots_txt=True,\n",
    "    request_delay=2.0,\n",
    "    max_retries=3,\n",
    "    timeout=30,\n",
    "    verify_ssl=True,\n",
    "    headers={\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Cache-Control\": \"max-age=0\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create a scraper with the custom configuration\n",
    "advanced_scraper = Scraper(config=config)\n",
    "\n",
    "# Scrape multiple websites\n",
    "print(\"Scraping multiple websites...\")\n",
    "urls = [\n",
    "    \"https://example.com\",\n",
    "    \"https://example.org\",\n",
    "    \"https://example.net\"\n",
    "]\n",
    "\n",
    "results = advanced_scraper.scrape_multiple(urls, schema)\n",
    "\n",
    "# Print the scraped data\n",
    "print(\"\\nScraped Data:\")\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nWebsite {i+1}:\")\n",
    "    print(f\"URL: {result.get('url')}\")\n",
    "    print(f\"Title: {result.get('title')}\")\n",
    "    print(f\"Description: {result.get('description')}\")\n",
    "    print(f\"Number of paragraphs: {len(result.get('paragraphs', []))}\")\n",
    "    print(f\"Number of links: {len(result.get('links', []))}\")\n",
    "    print(f\"Number of images: {len(result.get('images', []))}\")\n",
    "\n",
    "# Export the data to various formats\n",
    "print(\"\\nExporting data...\")\n",
    "advanced_scraper.export(results, \"/content/web-scraper-toolkit/data/multiple_examples.json\")\n",
    "advanced_scraper.export(results, \"/content/web-scraper-toolkit/data/multiple_examples.csv\")\n",
    "\n",
    "# Close the scraper\n",
    "advanced_scraper.close()\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. News Website Scraping Example\n",
    "\n",
    "Let's create a more practical example by scraping headlines from a news website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scraper for news websites\n",
    "news_scraper = Scraper(engine=\"requests\")\n",
    "\n",
    "# Define the schema for BBC News\n",
    "bbc_schema = {\n",
    "    \"headlines\": {\n",
    "        \"selector\": \".gs-c-promo-heading__title\",\n",
    "        \"multiple\": True\n",
    "    },\n",
    "    \"summaries\": {\n",
    "        \"selector\": \".gs-c-promo-summary\",\n",
    "        \"multiple\": True\n",
    "    },\n",
    "    \"article_links\": {\n",
    "        \"selector\": \".gs-c-promo-heading\",\n",
    "        \"attribute\": \"href\",\n",
    "        \"multiple\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Scrape BBC News\n",
    "print(\"Scraping BBC News...\")\n",
    "try:\n",
    "    bbc_data = news_scraper.scrape(\"https://www.bbc.com/news\", bbc_schema)\n",
    "    \n",
    "    # Print the headlines\n",
    "    print(\"\\nBBC News Headlines:\")\n",
    "    for i, headline in enumerate(bbc_data.get('headlines', [])):\n",
    "        if i < 10:  # Limit to 10 headlines\n",
    "            print(f\"{i+1}. {headline}\")\n",
    "    \n",
    "    # Export the data\n",
    "    news_scraper.export(bbc_data, \"/content/web-scraper-toolkit/data/bbc_news.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Error scraping BBC News: {str(e)}\")\n",
    "    print(\"Note: Website structure may have changed or access might be restricted.\")\n",
    "\n",
    "# Define the schema for CNN\n",
    "cnn_schema = {\n",
    "    \"headlines\": {\n",
    "        \"selector\": \".container__headline\",\n",
    "        \"multiple\": True\n",
    "    },\n",
    "    \"summaries\": {\n",
    "        \"selector\": \".container__description\",\n",
    "        \"multiple\": True\n",
    "    },\n",
    "    \"article_links\": {\n",
    "        \"selector\": \".container__link\",\n",
    "        \"attribute\": \"href\",\n",
    "        \"multiple\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Scrape CNN\n",
    "print(\"\\nScraping CNN...\")\n",
    "try:\n",
    "    cnn_data = news_scraper.scrape(\"https://www.cnn.com\", cnn_schema)\n",
    "    \n",
    "    # Print the headlines\n",
    "    print(\"\\nCNN Headlines:\")\n",
    "    for i, headline in enumerate(cnn_data.get('headlines', [])):\n",
    "        if i < 10:  # Limit to 10 headlines\n",
    "            print(f\"{i+1}. {headline}\")\n",
    "    \n",
    "    # Export the data\n",
    "    news_scraper.export(cnn_data, \"/content/web-scraper-toolkit/data/cnn_news.json\")\n",
    "except Exception as e:\n",
    "    print(f\"Error scraping CNN: {str(e)}\")\n",
    "    print(\"Note: Website structure may have changed or access might be restricted.\")\n",
    "\n",
    "# Close the scraper\n",
    "news_scraper.close()\n",
    "\n",
    "print(\"\\nNews scraping complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. E-commerce Price Monitoring Example\n",
    "\n",
    "Let's create a simplified version of the e-commerce price monitoring example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "\n",
    "# Create a scraper for e-commerce websites\n",
    "ecommerce_scraper = Scraper(engine=\"requests\")\n",
    "\n",
    "# Define a function to clean price strings\n",
    "def clean_price(price_str):\n",
    "    if not price_str:\n",
    "        return None\n",
    "    # Remove currency symbols and commas\n",
    "    cleaned = price_str.replace(\"$\", \"\").replace(\"€\", \"\").replace(\"£\", \"\").replace(\",\", \"\")\n",
    "    # Extract the first number found\n",
    "    import re\n",
    "    match = re.search(r\"(\\d+\\.\\d+|\\d+)\", cleaned)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "    return None\n",
    "\n",
    "# Define product URLs to monitor (using example.com for demonstration)\n",
    "# In a real scenario, you would use actual e-commerce websites\n",
    "products = [\n",
    "    {\n",
    "        \"name\": \"Example Product\",\n",
    "        \"url\": \"https://example.com\",\n",
    "        \"selector_schema\": {\n",
    "            \"title\": \"h1\",\n",
    "            \"price\": {\n",
    "                \"selector\": \"p\",  # Using paragraph as a placeholder for price\n",
    "                \"processors\": [clean_price]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Current timestamp\n",
    "timestamp = datetime.datetime.now().isoformat()\n",
    "\n",
    "# Load existing price history or create a new one\n",
    "price_history_file = \"/content/web-scraper-toolkit/data/price_history.json\"\n",
    "try:\n",
    "    with open(price_history_file, 'r') as f:\n",
    "        price_history = json.load(f)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    price_history = {}\n",
    "\n",
    "# Scrape each product\n",
    "for product in products:\n",
    "    product_name = product[\"name\"]\n",
    "    product_url = product[\"url\"]\n",
    "    selector_schema = product[\"selector_schema\"]\n",
    "    \n",
    "    print(f\"Scraping product: {product_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Scrape the product page\n",
    "        data = ecommerce_scraper.scrape(product_url, selector_schema)\n",
    "        \n",
    "        # Add timestamp and URL\n",
    "        data[\"timestamp\"] = timestamp\n",
    "        data[\"url\"] = product_url\n",
    "        \n",
    "        # Initialize price history for this product if it doesn't exist\n",
    "        if product_name not in price_history:\n",
    "            price_history[product_name] = []\n",
    "        \n",
    "        # Add the current price to the history\n",
    "        price_history[product_name].append({\n",
    "            \"timestamp\": timestamp,\n",
    "            \"price\": data.get(\"price\"),\n",
    "            \"title\": data.get(\"title\")\n",
    "        })\n",
    "        \n",
    "        # Print the current data\n",
    "        print(f\"Title: {data.get('title')}\")\n",
    "        print(f\"Price: {data.get('price')}\")\n",
    "        \n",
    "        # Export the current product data\n",
    "        product_file = f\"/content/web-scraper-toolkit/data/{product_name.lower().replace(' ', '_')}.json\"\n",
    "        ecommerce_scraper.export(data, product_file)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {product_name}: {str(e)}\")\n",
    "\n",
    "# Save the updated price history\n",
    "with open(price_history_file, 'w') as f:\n",
    "    json.dump(price_history, f, indent=2)\n",
    "\n",
    "# Close the scraper\n",
    "ecommerce_scraper.close()\n",
    "\n",
    "print(\"\\nE-commerce price monitoring complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use the Web Scraper Toolkit in Google Colab. We've covered:\n",
    "\n",
    "1. Setting up the environment\n",
    "2. Basic scraping\n",
    "3. Advanced scraping with custom configuration\n",
    "4. News website scraping\n",
    "5. E-commerce price monitoring\n",
    "\n",
    "The Web Scraper Toolkit provides a flexible and powerful framework for web scraping tasks, and it can be easily used in Google Colab for quick prototyping and analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}