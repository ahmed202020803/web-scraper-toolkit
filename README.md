# Web Scraper Toolkit

A comprehensive toolkit for scraping, processing, and analyzing web data with various tools and utilities.

## ğŸš€ Features

- **Multiple Scraping Engines**: Support for Selenium, BeautifulSoup, Scrapy, and Playwright
- **Proxy Management**: Rotate proxies to avoid IP bans and rate limiting
- **Data Extraction**: Extract structured data from HTML, JSON, XML, and JavaScript-rendered pages
- **Data Processing**: Clean, transform, and normalize scraped data
- **Data Storage**: Export to CSV, JSON, Excel, or databases (SQLite, PostgreSQL, MongoDB)
- **Scheduling**: Schedule scraping jobs at regular intervals
- **Monitoring**: Monitor scraping jobs and receive notifications
- **Rate Limiting**: Respect website robots.txt and implement polite scraping
- **Captcha Handling**: Solve common captchas and anti-bot challenges
- **User-Agent Rotation**: Rotate user agents to mimic different browsers
- **Headless Browsing**: Support for headless Chrome and Firefox
- **API Integration**: Extract data from REST and GraphQL APIs
- **Parallel Processing**: Scrape multiple pages simultaneously
- **Error Handling**: Robust error handling and retry mechanisms
- **Logging**: Comprehensive logging for debugging and auditing

## ğŸ“‹ Installation

```bash
# Clone the repository
git clone https://github.com/ahmed202020803/web-scraper-toolkit.git
cd web-scraper-toolkit

# Create a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install browser drivers (for Selenium)
python scripts/install_drivers.py
```

## ğŸ”§ Configuration

Create a `.env` file in the project root with your configuration:

```
# General settings
LOG_LEVEL=INFO
DATA_DIR=./data

# Database settings
DB_TYPE=sqlite  # sqlite, postgres, mongodb
DB_PATH=./data/scraper.db  # For SQLite
# DB_HOST=localhost  # For PostgreSQL/MongoDB
# DB_PORT=5432  # For PostgreSQL/MongoDB
# DB_USER=user  # For PostgreSQL/MongoDB
# DB_PASSWORD=password  # For PostgreSQL/MongoDB
# DB_NAME=scraper  # For PostgreSQL/MongoDB

# Proxy settings
USE_PROXIES=false
PROXY_LIST_PATH=./config/proxies.txt
PROXY_ROTATION_POLICY=round-robin  # round-robin, random

# Browser settings
BROWSER_TYPE=chrome  # chrome, firefox
HEADLESS=true
USER_AGENT_ROTATION=true
USER_AGENT_LIST_PATH=./config/user_agents.txt

# Rate limiting
RESPECT_ROBOTS_TXT=true
REQUEST_DELAY=2  # seconds between requests
MAX_REQUESTS_PER_MINUTE=30

# Captcha settings
SOLVE_CAPTCHAS=false
# CAPTCHA_SERVICE=2captcha  # 2captcha, anticaptcha
# CAPTCHA_API_KEY=your_api_key
```

## ğŸ› ï¸ Usage

### Basic Usage

```python
from web_scraper_toolkit import Scraper

# Initialize a scraper
scraper = Scraper(engine="selenium")

# Scrape a website
data = scraper.scrape("https://example.com", {
    "title": "h1",
    "paragraphs": "p",
    "links": {"selector": "a", "attribute": "href"}
})

# Export the data
scraper.export(data, "example_data.json")
```

### Advanced Usage

```python
from web_scraper_toolkit import Scraper, ScraperConfig
from web_scraper_toolkit.processors import TextCleaner, DateNormalizer
from web_scraper_toolkit.exporters import PostgreSQLExporter

# Configure the scraper
config = ScraperConfig(
    engine="playwright",
    browser="firefox",
    headless=True,
    use_proxies=True,
    proxy_rotation_policy="random",
    respect_robots_txt=True,
    request_delay=3,
    max_retries=5
)

# Initialize the scraper with the configuration
scraper = Scraper(config=config)

# Define data processors
processors = [
    TextCleaner(remove_html=True, lowercase=False),
    DateNormalizer(format="%Y-%m-%d")
]

# Define the data schema
schema = {
    "title": {"selector": "h1", "processors": [processors[0]]},
    "date": {"selector": ".date", "processors": [processors[0], processors[1]]},
    "content": {"selector": ".content", "processors": [processors[0]]},
    "categories": {"selector": ".category", "multiple": True},
    "image_urls": {"selector": "img", "attribute": "src", "multiple": True}
}

# Scrape multiple pages
urls = [
    "https://example.com/page1",
    "https://example.com/page2",
    "https://example.com/page3"
]

# Scrape with the schema
results = scraper.scrape_multiple(urls, schema)

# Export to PostgreSQL
exporter = PostgreSQLExporter(
    host="localhost",
    port=5432,
    user="user",
    password="password",
    database="scraper"
)
exporter.export(results, "scraped_data")
```

### Scheduling Jobs

```python
from web_scraper_toolkit import ScraperJob, JobScheduler

# Define a job
job = ScraperJob(
    name="Daily News Scraper",
    url="https://news-site.com",
    schema={
        "headlines": {"selector": ".headline", "multiple": True},
        "summaries": {"selector": ".summary", "multiple": True}
    },
    exporter="json",
    export_path="./data/daily_news.json"
)

# Create a scheduler
scheduler = JobScheduler()

# Add the job to run daily at 8:00 AM
scheduler.add_job(job, "daily", "08:00")

# Start the scheduler
scheduler.start()
```

## ğŸ“ Project Structure

```
web-scraper-toolkit/
â”œâ”€â”€ config/                  # Configuration files
â”‚   â”œâ”€â”€ proxies.txt          # List of proxy servers
â”‚   â””â”€â”€ user_agents.txt      # List of user agents
â”œâ”€â”€ data/                    # Directory for storing scraped data
â”œâ”€â”€ docs/                    # Documentation
â”œâ”€â”€ examples/                # Example scripts
â”‚   â”œâ”€â”€ basic_scraping.py
â”‚   â”œâ”€â”€ api_scraping.py
â”‚   â”œâ”€â”€ e-commerce_scraper.py
â”‚   â””â”€â”€ news_aggregator.py
â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â””â”€â”€ install_drivers.py   # Script to install browser drivers
â”œâ”€â”€ tests/                   # Test suite
â”œâ”€â”€ web_scraper_toolkit/     # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper.py           # Core scraper class
â”‚   â”œâ”€â”€ config.py            # Configuration handling
â”‚   â”œâ”€â”€ engines/             # Scraping engines
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ selenium_engine.py
â”‚   â”‚   â”œâ”€â”€ bs4_engine.py
â”‚   â”‚   â”œâ”€â”€ scrapy_engine.py
â”‚   â”‚   â””â”€â”€ playwright_engine.py
â”‚   â”œâ”€â”€ processors/          # Data processors
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ text_processors.py
â”‚   â”‚   â”œâ”€â”€ date_processors.py
â”‚   â”‚   â””â”€â”€ image_processors.py
â”‚   â”œâ”€â”€ exporters/           # Data exporters
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ csv_exporter.py
â”‚   â”‚   â”œâ”€â”€ json_exporter.py
â”‚   â”‚   â”œâ”€â”€ excel_exporter.py
â”‚   â”‚   â”œâ”€â”€ sqlite_exporter.py
â”‚   â”‚   â”œâ”€â”€ postgres_exporter.py
â”‚   â”‚   â””â”€â”€ mongodb_exporter.py
â”‚   â”œâ”€â”€ utils/               # Utility functions
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ proxy_manager.py
â”‚   â”‚   â”œâ”€â”€ user_agent_manager.py
â”‚   â”‚   â”œâ”€â”€ robots_txt.py
â”‚   â”‚   â””â”€â”€ captcha_solver.py
â”‚   â”œâ”€â”€ scheduler/           # Job scheduling
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ job.py
â”‚   â”‚   â””â”€â”€ scheduler.py
â”‚   â””â”€â”€ monitoring/          # Monitoring and notifications
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ notifier.py
â”œâ”€â”€ .env.example             # Example environment variables
â”œâ”€â”€ .gitignore               # Git ignore file
â”œâ”€â”€ LICENSE                  # License file
â”œâ”€â”€ README.md                # Project documentation
â””â”€â”€ requirements.txt         # Python dependencies
```

## ğŸ“Š Example Use Cases

1. **E-commerce Price Monitoring**: Track product prices across multiple websites
2. **News Aggregation**: Collect news articles from various sources
3. **Social Media Analysis**: Gather data from social media platforms
4. **Research Data Collection**: Collect data for academic research
5. **Job Listing Aggregation**: Gather job listings from multiple job boards
6. **Real Estate Data Collection**: Track property listings and prices
7. **Weather Data Collection**: Gather weather data from multiple sources
8. **Financial Data Analysis**: Collect stock prices and financial news

## ğŸ”’ Legal Considerations

Always ensure you're complying with:
- Website Terms of Service
- robots.txt directives
- Rate limiting and politeness
- Data privacy laws (GDPR, CCPA, etc.)
- Copyright and intellectual property laws

This toolkit is provided for educational and legitimate research purposes only.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.